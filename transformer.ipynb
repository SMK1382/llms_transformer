{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86d335a7-f767-4596-ba1c-973eadab6976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from word_embedding import embedding\n",
    "from vocabulary import vocabulary\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ad2250c-9ed3-4224-bf51-b7b19dfd5367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(X):\n",
    "    batch, seq_len, d_model = X.shape\n",
    "    for i in range(batch):\n",
    "        for pos in range(seq_len):\n",
    "            for j in range(0, d_model, 2):\n",
    "                X[i, pos, j] += np.sin(pos / (10000 ** (2 * j / d_model)))\n",
    "            for j in range(1, d_model, 2):\n",
    "                X[i, pos, j] += np.cos(pos / (10000 ** (2 * (j-1) / d_model)))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c524dd16-461c-40c0-a257-99a9dd11f523",
   "metadata": {},
   "outputs": [],
   "source": [
    "class softmax:\n",
    "    def __init__(self):\n",
    "        self.probs = None\n",
    "\n",
    "    def softmax_forward(self, x, axis=-1):\n",
    "        x_shifted = x - np.max(x, axis=axis, keepdims=True)\n",
    "        x_shifted = np.clip(x_shifted, -500, 500)\n",
    "        exp_x = np.exp(x_shifted)\n",
    "        self.probs = exp_x / (np.sum(exp_x, axis=axis, keepdims=True) + 1e-10)\n",
    "        return self.probs\n",
    "\n",
    "    def softmax_backward(self, grad_x):\n",
    "        temp = np.sum(self.probs * grad_x, axis=-1, keepdims=True)\n",
    "        dS = self.probs * (grad_x - temp)\n",
    "        return dS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ec112e2-86eb-48df-bff9-5ee86d287931",
   "metadata": {},
   "outputs": [],
   "source": [
    "class add_and_norm:\n",
    "    def __init__(self, d_model, eps=1e-5, learning_rate=0.01):\n",
    "        self.X = None\n",
    "        self.gamma = np.ones(d_model)\n",
    "        self.beta = np.zeros(d_model)\n",
    "        self.eps = eps\n",
    "        self.alpha = learning_rate\n",
    "\n",
    "        self.X_hat = None\n",
    "        self.mean = None\n",
    "        self.var = None\n",
    "        self.Z = None\n",
    "\n",
    "    def get_input(self, X):\n",
    "        self.X = X\n",
    "        self.batch_size, self.seq_len, self.d_model = X.shape\n",
    "\n",
    "    def forward(self, sublayer_out):\n",
    "        Z = self.X + sublayer_out\n",
    "\n",
    "        mean = np.mean(Z, axis=-1, keepdims=True)\n",
    "        var = np.var(Z, axis=-1, keepdims=True)\n",
    "\n",
    "        X_hat = (Z - mean) / np.sqrt(var + self.eps)\n",
    "        out = self.gamma * X_hat + self.beta\n",
    "\n",
    "        self.Z = Z\n",
    "        self.mean = mean\n",
    "        self.var = var\n",
    "        self.X_hat = X_hat\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        d_gamma = np.sum(d_out * self.X_hat, axis=(0, 1))\n",
    "        d_beta = np.sum(d_out, axis=(0, 1))\n",
    "\n",
    "        dX_hat = d_out * self.gamma\n",
    "        std_inv = 1.0 / np.sqrt(self.var + self.eps)\n",
    "\n",
    "        d_var = np.sum(dX_hat * (self.Z - self.mean) * -0.5 * std_inv**3, axis=-1, keepdims=True)\n",
    "        d_mean = np.sum(dX_hat * -std_inv, axis=-1, keepdims=True) + d_var * np.mean(-2.0 * (self.Z - self.mean), axis=-1, keepdims=True)\n",
    "        d_Z = dX_hat * std_inv + d_var * 2.0 * (self.Z - self.mean) / self.d_model + d_mean / self.d_model\n",
    "\n",
    "        d_X = d_Z\n",
    "        d_Sublayer = d_Z\n",
    "\n",
    "        self.gamma -= self.alpha * d_gamma\n",
    "        self.beta -= self.alpha * d_beta\n",
    "\n",
    "        return d_X, d_Sublayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93207941-1510-4d48-8c93-ba37e211d4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_head_attention:\n",
    "    def __init__(self, d_model, n_head, learning_rate=0.01, decoder=False, mask=False):\n",
    "        scale = 0.01\n",
    "        self.W_Q = np.random.randn(d_model, d_model) * scale\n",
    "        self.W_K = np.random.randn(d_model, d_model) * scale\n",
    "        self.W_V = np.random.randn(d_model, d_model) * scale\n",
    "        self.W_O = np.random.randn(d_model, d_model) * scale\n",
    "        self.alph = learning_rate\n",
    "        self.d_head = d_model // n_head\n",
    "        self.n_head = n_head\n",
    "        self.mask = mask\n",
    "        self.decoder = decoder\n",
    "        self.softmax = softmax()\n",
    "\n",
    "        self.X = None\n",
    "        self.Q = None\n",
    "        self.K = None\n",
    "        self.V = None\n",
    "        self.attn = None\n",
    "        self.out_heads = None\n",
    "        self.out_cat = None\n",
    "\n",
    "    def get_input(self, X):\n",
    "        self.X = X\n",
    "        if self.decoder and isinstance(X, (list, tuple)):\n",
    "            self.batch_size, self.seq_len, self.d_model = X[0].shape\n",
    "        else:\n",
    "            self.batch_size, self.seq_len, self.d_model = X.shape\n",
    "\n",
    "    def attention_forward(self, padding_mask):\n",
    "        if self.decoder and isinstance(self.X, (list, tuple)):\n",
    "            Q = self.X[0] @ self.W_Q\n",
    "            K = self.X[0] @ self.W_K\n",
    "            V = self.X[1] @ self.W_V\n",
    "        else:\n",
    "            Q = self.X @ self.W_Q\n",
    "            K = self.X @ self.W_K\n",
    "            V = self.X @ self.W_V\n",
    "\n",
    "        Q = Q.reshape(self.batch_size, self.seq_len, self.n_head, self.d_head).transpose(0, 2, 1, 3)\n",
    "        K = K.reshape(self.batch_size, self.seq_len, self.n_head, self.d_head).transpose(0, 2, 1, 3)\n",
    "        V = V.reshape(self.batch_size, self.seq_len, self.n_head, self.d_head).transpose(0, 2, 1, 3)\n",
    "\n",
    "        scores = Q @ K.transpose(0, 1, 3, 2) / np.sqrt(self.d_head)\n",
    "\n",
    "        if padding_mask is not None:\n",
    "            mask_expanded = padding_mask[:, np.newaxis, np.newaxis, :]\n",
    "            scores = np.where(mask_expanded == 1, scores, -1e9)\n",
    "\n",
    "        if self.mask:\n",
    "            causal_mask = np.tril(np.ones((self.seq_len, self.seq_len)))\n",
    "            scores = np.where(causal_mask == 1, scores, -1e9)\n",
    "\n",
    "        attn = self.softmax.softmax_forward(scores, axis=-1)\n",
    "        out_heads = attn @ V\n",
    "        out_cat = out_heads.transpose(0, 2, 1, 3).reshape(self.batch_size, self.seq_len, self.d_model)\n",
    "        out = out_cat @ self.W_O\n",
    "\n",
    "        self.Q = Q\n",
    "        self.K = K\n",
    "        self.V = V\n",
    "        self.attn = attn\n",
    "        self.out_heads = out_heads\n",
    "        self.out_cat = out_cat\n",
    "\n",
    "        return out\n",
    "\n",
    "    def attention_backward(self, d_out):\n",
    "        B, N, D = self.batch_size, self.seq_len, self.d_model\n",
    "        H = self.n_head\n",
    "        Dh = self.d_head\n",
    "        scale = np.sqrt(self.d_head)\n",
    "\n",
    "        d_out_cat = d_out @ self.W_O.T\n",
    "        dW_O = (self.out_cat.reshape(B * N, D).T @ d_out.reshape(B * N, D))\n",
    "\n",
    "        d_out_heads = d_out_cat.reshape(B, N, H, Dh).transpose(0, 2, 1, 3)\n",
    "\n",
    "        d_V = self.attn.transpose(0, 1, 3, 2) @ d_out_heads\n",
    "        d_attn = d_out_heads @ self.V.transpose(0, 1, 3, 2)\n",
    "\n",
    "        d_scores = self.softmax.softmax_backward(d_attn)\n",
    "\n",
    "        d_Q = (d_scores @ self.K) / scale\n",
    "        d_K = (d_scores.transpose(0, 1, 3, 2) @ self.Q) / scale\n",
    "\n",
    "        d_Q = d_Q.transpose(0, 2, 1, 3).reshape(B * N, D)\n",
    "        d_K = d_K.transpose(0, 2, 1, 3).reshape(B * N, D)\n",
    "        d_V = d_V.transpose(0, 2, 1, 3).reshape(B * N, D)\n",
    "\n",
    "        X_flat = self.X.reshape(B * N, D) if not isinstance(self.X, (list, tuple)) else self.X[0].reshape(B * N, D)\n",
    "\n",
    "        dW_Q = X_flat.T @ d_Q\n",
    "        dW_K = X_flat.T @ d_K\n",
    "        dW_V = X_flat.T @ d_V\n",
    "\n",
    "        self.W_Q -= self.alph * dW_Q\n",
    "        self.W_K -= self.alph * dW_K\n",
    "        self.W_V -= self.alph * dW_V\n",
    "        self.W_O -= self.alph * dW_O\n",
    "\n",
    "        d_X = (d_Q @ self.W_Q.T + d_K @ self.W_K.T + d_V @ self.W_V.T).reshape(B, N, D)\n",
    "\n",
    "        if self.decoder:\n",
    "            return (d_Q @ self.W_Q.T + d_K @ self.W_K.T).reshape(B, N, D), (d_V @ self.W_V.T).reshape(B, N, D)\n",
    "\n",
    "        return d_X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87ebd28d-5104-4d3a-ab1b-81a5a71742d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class feed_forward:\n",
    "    def __init__(self, d_model, d_ff=8, learning_rate=0.01):\n",
    "        scale = 0.01\n",
    "        self.W_1 = np.random.randn(d_model, d_ff) * scale\n",
    "        self.b_1 = np.zeros(d_ff)\n",
    "        self.W_2 = np.random.randn(d_ff, d_model) * scale\n",
    "        self.b_2 = np.zeros(d_model)\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.X = None\n",
    "\n",
    "        self.Z1 = None\n",
    "        self.A1 = None\n",
    "\n",
    "    def get_input(self, X):\n",
    "        self.X = X\n",
    "        self.batch_size, self.seq_len, self.d_model = X.shape\n",
    "\n",
    "    def forward(self):\n",
    "        X_flat = self.X.reshape(-1, self.d_model)\n",
    "\n",
    "        self.Z1 = X_flat @ self.W_1 + self.b_1\n",
    "        self.A1 = np.maximum(0, self.Z1)\n",
    "\n",
    "        out = self.A1 @ self.W_2 + self.b_2\n",
    "        out = out.reshape(self.batch_size, self.seq_len, self.d_model)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dOut):\n",
    "        B, N, D = self.batch_size, self.seq_len, self.d_model\n",
    "\n",
    "        dOut_flat = dOut.reshape(-1, D)\n",
    "\n",
    "        dW_2 = self.A1.T @ dOut_flat\n",
    "        db_2 = np.sum(dOut_flat, axis=0)\n",
    "\n",
    "        dA1 = dOut_flat @ self.W_2.T\n",
    "        dZ1 = dA1 * (self.Z1 > 0)\n",
    "\n",
    "        X_flat = self.X.reshape(-1, D)\n",
    "        dW_1 = X_flat.T @ dZ1\n",
    "        db_1 = np.sum(dZ1, axis=0)\n",
    "\n",
    "        dX = dZ1 @ self.W_1.T\n",
    "        dX = dX.reshape(B, N, D)\n",
    "\n",
    "        self.W_1 -= self.learning_rate * dW_1\n",
    "        self.b_1 -= self.learning_rate * db_1\n",
    "        self.W_2 -= self.learning_rate * dW_2\n",
    "        self.b_2 -= self.learning_rate * db_2\n",
    "\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f80582ef-1964-4458-8795-e5b208e6121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, d_model, d_out, learning_rate=0.01):\n",
    "        scale = 0.01\n",
    "        self.W = np.random.randn(d_model, d_out) * scale\n",
    "        self.b = np.zeros(d_out)\n",
    "        self.lr = learning_rate\n",
    "\n",
    "        self.X = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.batch_size, self.seq_len, self.d_model = X.shape\n",
    "\n",
    "        X_flat = X.reshape(-1, self.d_model)\n",
    "        out = X_flat @ self.W + self.b\n",
    "        out = out.reshape(self.batch_size, self.seq_len, -1)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        B, N, D_out = d_out.shape\n",
    "        D_in = self.W.shape[0]\n",
    "\n",
    "        d_out_flat = d_out.reshape(-1, D_out)\n",
    "        X_flat = self.X.reshape(-1, D_in)\n",
    "\n",
    "        dW = X_flat.T @ d_out_flat\n",
    "        db = np.sum(d_out_flat, axis=0)\n",
    "        dX = d_out_flat @ self.W.T\n",
    "\n",
    "        self.W -= self.lr * dW\n",
    "        self.b -= self.lr * db\n",
    "\n",
    "        return dX.reshape(B, N, D_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49c5acb2-e7a7-43f2-a38c-51b63251f9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(x):\n",
    "    temp = []\n",
    "    max_len = max(len(s) for s in x)\n",
    "    for i in x:\n",
    "        temp.append(i + ['<pad>'] * (max_len - len(i)))\n",
    "    return np.array(temp), np.where(np.array(temp) == '<pad>', 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a497812-c034-4025-a7df-cfe13b7042bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_with_grad(vocabulary, target_model, output_model):\n",
    "    B, T, V = output_model.shape\n",
    "\n",
    "    target_index = []\n",
    "    for i in target_model:\n",
    "        for j in i:\n",
    "            target_index.append(np.where(vocabulary == j)[0][0])\n",
    "    target_index = np.array(target_index)\n",
    "\n",
    "    output_flat = output_model.reshape(B * T, V)\n",
    "    output_flat = np.clip(output_flat, 1e-10, 1.0)\n",
    "\n",
    "    loss = -np.sum(np.log(output_flat[np.arange(B * T), target_index] + 1e-10))\n",
    "\n",
    "    grad = output_flat.copy()\n",
    "    grad[np.arange(B * T), target_index] -= 1\n",
    "    grad = grad / (B * T)\n",
    "    grad = grad.reshape(B, T, V)\n",
    "\n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82f4ff63-95ac-4944-8e02-a7525becd970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_embedding(input_model, embedding):\n",
    "    result = np.zeros((input_model.shape[0], input_model.shape[1], len(list(embedding.values())[0])))\n",
    "    for b in range(input_model.shape[0]):\n",
    "        for s in range(input_model.shape[1]):\n",
    "            word = input_model[b, s]\n",
    "            for key, value in embedding.items():\n",
    "                if word == key:\n",
    "                    result[b, s] = value\n",
    "                    break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8201df3-a403-4930-b71a-c37fdf94339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_train(X, target_model, vocabulary, padding_matrix,\n",
    "                  multi_head_attention_1, multi_head_attention_2, masked_multi_head_attention,\n",
    "                  feed_forward_1, feed_forward_2, norm_1, norm_2, norm_3, norm_4, norm_5,\n",
    "                  linear, softmax_layer):\n",
    "\n",
    "    # Encoder\n",
    "    X_1 = X_2 = positional_encoding(X.copy())\n",
    "    multi_head_attention_1.get_input(X_2)\n",
    "    X_2 = multi_head_attention_1.attention_forward(padding_matrix)\n",
    "    norm_1.get_input(X_2)\n",
    "    X_1 = X_2 = norm_1.forward(X_1)\n",
    "\n",
    "    feed_forward_1.get_input(X_2)\n",
    "    X_2 = feed_forward_1.forward()\n",
    "    norm_2.get_input(X_2)\n",
    "    output_encoder = norm_2.forward(X_1)\n",
    "\n",
    "    # Decoder\n",
    "    X_1 = X_2 = positional_encoding(X.copy())\n",
    "    masked_multi_head_attention.get_input(X_2)\n",
    "    X_2 = masked_multi_head_attention.attention_forward(padding_matrix)\n",
    "    norm_3.get_input(X_2)\n",
    "    X_1 = X_2 = norm_3.forward(X_1)\n",
    "\n",
    "    multi_head_attention_2.get_input([output_encoder, X_2])\n",
    "    X_2 = multi_head_attention_2.attention_forward(padding_matrix)\n",
    "    norm_4.get_input(X_2)\n",
    "    X_1 = X_2 = norm_4.forward(X_1)\n",
    "\n",
    "    feed_forward_2.get_input(X_2)\n",
    "    X_2 = feed_forward_2.forward()\n",
    "    norm_5.get_input(X_2)\n",
    "    X_2 = norm_5.forward(X_1)\n",
    "\n",
    "    X_1 = linear.forward(X_2)\n",
    "    X_1 = softmax_layer.softmax_forward(X_1)\n",
    "\n",
    "    return cross_entropy_with_grad(vocabulary, target_model, X_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d24e8362-b118-4626-bbe2-a2d56e78fa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_train(grad, \n",
    "                   multi_head_attention_1, multi_head_attention_2, masked_multi_head_attention,\n",
    "                   feed_forward_1, feed_forward_2, norm_1, norm_2, norm_3, norm_4, norm_5,\n",
    "                   linear, softmax_layer):\n",
    "\n",
    "    X = linear.backward(grad)\n",
    "\n",
    "    X, sub_layer = norm_5.backward(X)\n",
    "    X = feed_forward_2.backward(X)\n",
    "    X, sub_layer = norm_4.backward(X + sub_layer)\n",
    "#--------------------------------------------------------------------------------------\n",
    "    #X, sub_layer_2 = multi_head_attention_2.attention_backward(X + sub_layer)\n",
    "    #X, sub_layer = norm_3.backward(X + sub_layer_2)\n",
    "\n",
    "    #X = masked_multi_head_attention.attention_backward(X + sub_layer)\n",
    "    #X, sub_layer = norm_2.backward(X)\n",
    "\n",
    "    #X = feed_forward_1.backward(X)\n",
    "    #X, sub_layer = norm_1.backward(X + sub_layer)\n",
    "\n",
    "    #X = multi_head_attention_1.attention_backward(X + sub_layer)\n",
    "#--------------------------------------------------------------------------------------\n",
    "    X_1, sub_layer_2 = multi_head_attention_2.attention_backward(X)\n",
    "\n",
    "    X, sub_layer = norm_3.backward(sub_layer + sub_layer_2)\n",
    "    X = masked_multi_head_attention.attention_backward(X)\n",
    "    \n",
    "    X, sub_layer = norm_2.backward(X_1)\n",
    "    X = feed_forward_1.backward(X)\n",
    "    X, sub_layer = norm_1.backward(X + sub_layer)\n",
    "    X = multi_head_attention_1.attention_backward(X)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67b4c781-5f03-4782-98fd-c26969e17bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Loss: 63.1546\n",
      "Epoch 2/50 | Loss: 56.0460\n",
      "Epoch 3/50 | Loss: 49.1178\n",
      "Epoch 4/50 | Loss: 42.4977\n",
      "Epoch 5/50 | Loss: 36.3916\n",
      "Epoch 6/50 | Loss: 31.0519\n",
      "Epoch 7/50 | Loss: 26.6388\n",
      "Epoch 8/50 | Loss: 23.0865\n",
      "Epoch 9/50 | Loss: 20.1823\n",
      "Epoch 10/50 | Loss: 17.7388\n",
      "Epoch 11/50 | Loss: 15.6504\n",
      "Epoch 12/50 | Loss: 13.8661\n",
      "Epoch 13/50 | Loss: 12.3563\n",
      "Epoch 14/50 | Loss: 11.0940\n",
      "Epoch 15/50 | Loss: 10.0495\n",
      "Epoch 16/50 | Loss: 9.1903\n",
      "Epoch 17/50 | Loss: 8.4845\n",
      "Epoch 18/50 | Loss: 7.9028\n",
      "Epoch 19/50 | Loss: 7.4207\n",
      "Epoch 20/50 | Loss: 7.0179\n",
      "Epoch 21/50 | Loss: 6.6786\n",
      "Epoch 22/50 | Loss: 6.3902\n",
      "Epoch 23/50 | Loss: 6.1430\n",
      "Epoch 24/50 | Loss: 5.9292\n",
      "Epoch 25/50 | Loss: 5.7429\n",
      "Epoch 26/50 | Loss: 5.5795\n",
      "Epoch 27/50 | Loss: 5.4350\n",
      "Epoch 28/50 | Loss: 5.3066\n",
      "Epoch 29/50 | Loss: 5.1918\n",
      "Epoch 30/50 | Loss: 5.0885\n",
      "Epoch 31/50 | Loss: 4.9953\n",
      "Epoch 32/50 | Loss: 4.9107\n",
      "Epoch 33/50 | Loss: 4.8336\n",
      "Epoch 34/50 | Loss: 4.7630\n",
      "Epoch 35/50 | Loss: 4.6983\n",
      "Epoch 36/50 | Loss: 4.6386\n",
      "Epoch 37/50 | Loss: 4.5835\n",
      "Epoch 38/50 | Loss: 4.5324\n",
      "Epoch 39/50 | Loss: 4.4850\n",
      "Epoch 40/50 | Loss: 4.4408\n",
      "Epoch 41/50 | Loss: 4.3995\n",
      "Epoch 42/50 | Loss: 4.3608\n",
      "Epoch 43/50 | Loss: 4.3245\n",
      "Epoch 44/50 | Loss: 4.2904\n",
      "Epoch 45/50 | Loss: 4.2583\n",
      "Epoch 46/50 | Loss: 4.2281\n",
      "Epoch 47/50 | Loss: 4.1994\n",
      "Epoch 48/50 | Loss: 4.1723\n",
      "Epoch 49/50 | Loss: 4.1466\n",
      "Epoch 50/50 | Loss: 4.1223\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(4.1222571322263954)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class transformer:\n",
    "    def __init__(self, d_model, n_head, vocabulary, learning_rate=0.01):\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.multi_head_attention_1 = multi_head_attention(d_model, n_head, learning_rate)\n",
    "        self.multi_head_attention_2 = multi_head_attention(d_model, n_head, learning_rate, decoder=True)\n",
    "        self.masked_multi_head_attention = multi_head_attention(d_model, n_head, learning_rate, mask=True)\n",
    "\n",
    "        self.feed_forward_1 = feed_forward(d_model, learning_rate=learning_rate)\n",
    "        self.feed_forward_2 = feed_forward(d_model, learning_rate=learning_rate)\n",
    "\n",
    "        self.norm_1 = add_and_norm(d_model, learning_rate=learning_rate)\n",
    "        self.norm_2 = add_and_norm(d_model, learning_rate=learning_rate)\n",
    "        self.norm_3 = add_and_norm(d_model, learning_rate=learning_rate)\n",
    "        self.norm_4 = add_and_norm(d_model, learning_rate=learning_rate)\n",
    "        self.norm_5 = add_and_norm(d_model, learning_rate=learning_rate)\n",
    "\n",
    "        self.linear = Linear(d_model, len(vocabulary), learning_rate)\n",
    "        self.softmax = softmax()\n",
    "\n",
    "    def train(self, X, target_model, vocabulary, padding_matrix, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            loss, grad = forward_train(\n",
    "                X, target_model, vocabulary, padding_matrix,\n",
    "                self.multi_head_attention_1, self.multi_head_attention_2, self.masked_multi_head_attention,\n",
    "                self.feed_forward_1, self.feed_forward_2,\n",
    "                self.norm_1, self.norm_2, self.norm_3, self.norm_4, self.norm_5,\n",
    "                self.linear, self.softmax\n",
    "            )\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{epochs} | Loss: {loss:.4f}\")\n",
    "\n",
    "            backward_train(\n",
    "                grad,\n",
    "                self.multi_head_attention_1, self.multi_head_attention_2, self.masked_multi_head_attention,\n",
    "                self.feed_forward_1, self.feed_forward_2,\n",
    "                self.norm_1, self.norm_2, self.norm_3, self.norm_4, self.norm_5,\n",
    "                self.linear, self.softmax\n",
    "            )\n",
    "        return loss\n",
    "\n",
    "    def inference(self, X):\n",
    "        pass\n",
    "################################################################################################################\n",
    "batch_size = 3\n",
    "d_model = 512\n",
    "n_head = 8\n",
    "\n",
    "with open(\"samples.txt\", 'r') as file:\n",
    "    samples = file.read().lower().split(\".\\n\")\n",
    "    file.close()\n",
    "\n",
    "X = []\n",
    "targets = []\n",
    "for i in range(len(samples) - 1):\n",
    "    X.append([\"<bos>\"] + samples[i].split())\n",
    "    targets.append(samples[i].split() + [\"<eos>\"])\n",
    "\n",
    "input_model = []\n",
    "target_model = []\n",
    "for i in range(batch_size):\n",
    "    temp = random.randint(0, min(99, len(X) - 1))\n",
    "    input_model.append(X[temp])\n",
    "    target_model.append(targets[temp])\n",
    "\n",
    "input_model, padding_matrix = padding(input_model)\n",
    "target_model, _ = padding(target_model)\n",
    "\n",
    "input_model = input_embedding(input_model, embedding).reshape(input_model.shape[0], input_model.shape[1], d_model)\n",
    "\n",
    "transformer_model = transformer(d_model, n_head, vocabulary, learning_rate=0.01)\n",
    "transformer_model.train(input_model, target_model, vocabulary, padding_matrix, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55988d37-bb26-4711-856b-c1f7974241d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
